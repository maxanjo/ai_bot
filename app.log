INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:10:00] "OPTIONS /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/7S83rKjRzjWZWq5hgb3LaAP18DHse79P HTTP/1.1" 200 -
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
INFO:myapp:[{'is_related': 'no'}]
INFO:myapp:None
DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from projects/4/data\docstore.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/docstore.json
DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from projects/4/data\index_store.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/index_store.json
DEBUG:llama_index.vector_stores.simple:Loading llama_index.vector_stores.simple from projects/4/data\vector_store.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/vector_store.json
DEBUG:llama_index.graph_stores.simple:Loading llama_index.graph_stores.simple from projects/4/data\graph_store.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/graph_store.json
INFO:llama_index.indices.loading:Loading indices with ids: ['vector_index']
DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings
DEBUG:openai:api_version=None data='{"input": ["hello"], "model": "text-embedding-ada-002", "encoding_format": "base64"}' message='Post details'
DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/embeddings HTTP/1.1" 200 None
DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=16 request_id=930337dcde3b26713649566868d3b257 response_code=200
DEBUG:llama_index.indices.utils:> Top 1 nodes:
> [Node 909c5dd5-e87e-4999-a794-761f96e44168] [Similarity score:             -0.0301768] Welcome to TechHub, your ultimate destination for cutting-edge technology and innovation! At Tech...
DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions
DEBUG:openai:api_version=None data='{"messages": [{"role": "system", "content": "Context information is below.\\n--------------------\\nWelcome to TechHub, your ultimate destination for cutting-edge technology and innovation! At TechHub, we pride ourselves on being a premier online store dedicated to providing a curated selection of the latest and greatest in tech gadgets, including laptops, smartphones, and more.\\n\\nAbout Us:\\n\\nAt the heart of TechHub is a passion for technology and a commitment to delivering top-notch products that seamlessly blend style, functionality, and performance. With a team of tech enthusiasts, we understand the ever-evolving nature of the digital landscape and strive to stay at the forefront of the latest trends and advancements.\\n\\nOur Mission:\\n\\nOur mission at TechHub is to empower our customers with the best tools and devices that enhance their digital lifestyle. Whether you\'re a professional looking for a high-performance laptop to boost productivity or a smartphone aficionado seeking the latest flagship model, we\'ve got you covered. We believe that technology should not only meet but exceed your expectations, making every interaction with your devices a seamless and enjoyable experience.\\n\\nQuality Assurance:\\n\\nTechHub is committed to delivering products of the highest quality. We carefully curate our inventory, partnering with leading brands known for their commitment to excellence. Each product undergoes rigorous testing to ensure reliability, durability, and top-notch performance. Our dedication to quality assurance is reflected in every item we offer.\\n\\nCustomer-Centric Approach:\\n\\nAt TechHub, our customers are at the core of everything we do. We prioritize your satisfaction and strive to provide an unparalleled shopping experience. Our customer support team is ready to assist you at every step, from product inquiries to after-sales support. We value your feedback and continuously work to improve our services based on your needs.\\n\\nSecure Shopping:\\n\\nYour online security is our priority. TechHub employs state-of-the-art encryption technology to ensure that your personal information remains secure and protected. Shop with confidence, knowing that your data is in safe hands.\\n\\nExplore the world of innovation at TechHub, where technology meets lifestyle. Discover the perfect blend of style and functionality with our carefully curated selection of laptops, smartphones, and other cutting-edge gadgets. Join us on this exciting journey, and let\'s embrace the future together!\\n--------------------\\n\\u0412\\u044b \\u2014 \\u0418\\u0418 \\u0430\\u0441\\u0441\\u0438\\u0441\\u0442\\u0435\\u043d\\u0442 \\u043a\\u043e\\u043c\\u043f\\u0430\\u043d\\u0438\\u0438. \\u0412\\u0430\\u043c \\u0431\\u0443\\u0434\\u0443\\u0442 \\u0437\\u0430\\u0434\\u0430\\u043d\\u044b \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u044b \\u043e \\u043d\\u0430\\u0448\\u0435\\u0439 \\u043a\\u043e\\u043c\\u043f\\u0430\\u043d\\u0438\\u0438. \\u0412\\u0435\\u0436\\u043b\\u0438\\u0432\\u043e \\u043e\\u0442\\u0432\\u0435\\u0447\\u0430\\u0439\\u0442\\u0435 \\u043d\\u0430 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u044b.\\r\\n    \\u041d\\u0438\\u0436\\u0435 \\u043c\\u044b \\u043f\\u0440\\u0435\\u0434\\u043e\\u0441\\u0442\\u0430\\u0432\\u0438\\u043b\\u0438 \\u043a\\u043e\\u043d\\u0442\\u0435\\u043a\\u0441\\u0442\\u043d\\u0443\\u044e \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u044e.\\r\\n    ---------------------\\r\\n    {context_str}\\r\\n    ---------------------\\r\\n    \\u0423\\u0447\\u0438\\u0442\\u044b\\u0432\\u0430\\u044f \\u044d\\u0442\\u0443 \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u044e, \\u043e\\u0442\\u0432\\u0435\\u0442\\u044c\\u0442\\u0435, \\u043f\\u043e\\u0436\\u0430\\u043b\\u0443\\u0439\\u0441\\u0442\\u0430, \\u043d\\u0430 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441: {query_str}\\r\\n    . \\n Information about product"}, {"role": "user", "content": "hello"}], "stream": false, "model": "gpt-3.5-turbo", "temperature": 0, "max_tokens": null}' message='Post details'
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=506 request_id=e6e6279d0077bf471eaaf22a63ac226d response_code=200
INFO:myapp:{'id': 12, 'user_id': 12, 'name': 'Макс', 'company_name': None, 'website': 'asdf', 'description': None, 'index_id': 'hPCKiRuw8dKLRicw', 'token': 'e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625', 'open_ai_api_key': None, 'cohere_api_key': None, 'company_email': None, 'company_phone': None, 'created_at': datetime.datetime(2023, 12, 25, 13, 41, 57), 'updated_at': datetime.datetime(2023, 12, 31, 12, 5, 54), 'api': 1, 'product_data': '{"attributes": "color", "categories": "misc,smartphones"}', 'project_id': 4, 'status': 1, 'prompt': 'Вы — ИИ ассистент компании. Вам будут заданы вопросы о нашей компании. Вежливо отвечайте на вопросы.\r\n    Ниже мы предоставили контекстную информацию.\r\n    ---------------------\r\n    {context_str}\r\n    ---------------------\r\n    Учитывая эту информацию, ответьте, пожалуйста, на вопрос: {query_str}\r\n    ', 'model': 'gpt-3.5-turbo', 'response_mode': 'compact', 'top_k': None, 'temperature': Decimal('0.00'), 'max_tokens': 512, 'max_input_size': 1500, 'num_output': 256, 'max_chunk_overlap': 20, 'max_request_per_hour': None, 'common_max_request_per_day': None, 'email': 'maxanjo512@gmail.com', 'provider': None, 'role': 'user', 'email_verified_at': None, 'password': None, 'access_token': None, 'auth_token': None, 'remember_token': None, 'subscription_plan': 'free', 'spent_tokens': 0, 'left_tokens': 32912}
DEBUG:root:This is a debug message
DEBUG:celery.utils.functional:
def send_chat_request(user_id, project_id, session, total_tokens, answer):
    return 1

INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:10:05] "POST /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/7S83rKjRzjWZWq5hgb3LaAP18DHse79P HTTP/1.1" 200 -
DEBUG:root:This is a debug message
DEBUG:celery.utils.functional:
def unlock_chord(self, group_id, callback, interval=0, max_retries=1, result=2, Result=3, GroupResult=4, result_from_tuple=5, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def sendEmbeddingRequest(user_id, total_tokens):
    return 1

DEBUG:celery.utils.functional:
def group(self, tasks, result, group_id, partial_args, add_to_parent=0):
    return 1

DEBUG:celery.utils.functional:
def xmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def backend_cleanup():
    return 1

DEBUG:celery.utils.functional:
def chain(*args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def send_chat_request(user_id, project_id, session, total_tokens, answer):
    return 1

DEBUG:celery.utils.functional:
def xstarmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def accumulate(self, *args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def set_vector_index_task(self, token):
    return 1

DEBUG:celery.utils.functional:
def process_set_vector_index(self, token, task_id):
    return 1

DEBUG:celery.utils.functional:
def chord(self, header, body, partial_args=0, interval=1, countdown=2, max_retries=3, eager=4, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def chunks(task, it, n):
    return 1

DEBUG:myapp:Flask app started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
DEBUG:root:This is a debug message
DEBUG:celery.utils.functional:
def unlock_chord(self, group_id, callback, interval=0, max_retries=1, result=2, Result=3, GroupResult=4, result_from_tuple=5, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def sendEmbeddingRequest(user_id, total_tokens):
    return 1

DEBUG:celery.utils.functional:
def group(self, tasks, result, group_id, partial_args, add_to_parent=0):
    return 1

DEBUG:celery.utils.functional:
def xmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def backend_cleanup():
    return 1

DEBUG:celery.utils.functional:
def chain(*args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def send_chat_request(user_id, project_id, session, total_tokens, answer):
    return 1

DEBUG:celery.utils.functional:
def xstarmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def accumulate(self, *args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def set_vector_index_task(self, token):
    return 1

DEBUG:celery.utils.functional:
def process_set_vector_index(self, token, task_id):
    return 1

DEBUG:celery.utils.functional:
def chord(self, header, body, partial_args=0, interval=1, countdown=2, max_retries=3, eager=4, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def chunks(task, it, n):
    return 1

INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:20:29] "[33mGET /files/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625 HTTP/1.1[0m" 404 -
INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:20:32] "OPTIONS /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/tOc3f155FbtFZbO8O0uryFGgOBCi72Pm HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:20:32] "[33mPOST /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/tOc3f155FbtFZbO8O0uryFGgOBCi72Pm HTTP/1.1[0m" 404 -
DEBUG:myapp:Flask app started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
DEBUG:root:This is a debug message
DEBUG:celery.utils.functional:
def unlock_chord(self, group_id, callback, interval=0, max_retries=1, result=2, Result=3, GroupResult=4, result_from_tuple=5, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def sendEmbeddingRequest(user_id, total_tokens):
    return 1

DEBUG:celery.utils.functional:
def group(self, tasks, result, group_id, partial_args, add_to_parent=0):
    return 1

DEBUG:celery.utils.functional:
def xmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def backend_cleanup():
    return 1

DEBUG:celery.utils.functional:
def chain(*args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def send_chat_request(user_id, project_id, session, total_tokens, answer):
    return 1

DEBUG:celery.utils.functional:
def xstarmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def accumulate(self, *args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def set_vector_index_task(self, token):
    return 1

DEBUG:celery.utils.functional:
def process_set_vector_index(self, token, task_id):
    return 1

DEBUG:celery.utils.functional:
def chord(self, header, body, partial_args=0, interval=1, countdown=2, max_retries=3, eager=4, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def chunks(task, it, n):
    return 1

ERROR:myapp:Exception on /files/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625 [GET]
Traceback (most recent call last):
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\myapp.py", line 110, in list_files
    project_id = project['project_id']
                 ~~~~~~~^^^^^^^^^^^^^^
KeyError: 'project_id'
INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:21:29] "[35m[1mGET /files/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625 HTTP/1.1[0m" 500 -
INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:21:33] "OPTIONS /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/Mq6snXqUA3pmXreVBi3SRFqMz8pxu0rj HTTP/1.1" 200 -
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
INFO:myapp:[{'is_related': 'no'}]
INFO:myapp:None
ERROR:myapp:Exception on /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/Mq6snXqUA3pmXreVBi3SRFqMz8pxu0rj [POST]
Traceback (most recent call last):
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\env\Lib\site-packages\flask\app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\OSPanel\domains\ai_assistant\api\myapp.py", line 309, in get_project_details
    storageProject = f'{storage}{project["project_id"]}/data'
                                 ~~~~~~~^^^^^^^^^^^^^^
KeyError: 'project_id'
INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:21:34] "[35m[1mPOST /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/Mq6snXqUA3pmXreVBi3SRFqMz8pxu0rj HTTP/1.1[0m" 500 -
DEBUG:root:This is a debug message
DEBUG:celery.utils.functional:
def unlock_chord(self, group_id, callback, interval=0, max_retries=1, result=2, Result=3, GroupResult=4, result_from_tuple=5, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def set_vector_index_task(self, token):
    return 1

DEBUG:celery.utils.functional:
def process_set_vector_index(self, token, task_id):
    return 1

DEBUG:celery.utils.functional:
def group(self, tasks, result, group_id, partial_args, add_to_parent=0):
    return 1

DEBUG:celery.utils.functional:
def send_chat_request(user_id, project_id, session, total_tokens, answer):
    return 1

DEBUG:celery.utils.functional:
def xmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def backend_cleanup():
    return 1

DEBUG:celery.utils.functional:
def sendEmbeddingRequest(user_id, total_tokens):
    return 1

DEBUG:celery.utils.functional:
def chain(*args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def xstarmap(task, it):
    return 1

DEBUG:celery.utils.functional:
def accumulate(self, *args, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def chord(self, header, body, partial_args=0, interval=1, countdown=2, max_retries=3, eager=4, **kwargs):
    return 1

DEBUG:celery.utils.functional:
def chunks(task, it, n):
    return 1

DEBUG:myapp:Flask app started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:23:01] "OPTIONS /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/wck69kcLPjX9CFsDqNKufQIoXWuONpeD HTTP/1.1" 200 -
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
INFO:myapp:[{'is_related': 'no'}]
INFO:myapp:None
DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from projects/4/data\docstore.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/docstore.json
DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from projects/4/data\index_store.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/index_store.json
DEBUG:llama_index.vector_stores.simple:Loading llama_index.vector_stores.simple from projects/4/data\vector_store.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/vector_store.json
DEBUG:llama_index.graph_stores.simple:Loading llama_index.graph_stores.simple from projects/4/data\graph_store.json.
DEBUG:fsspec.local:open file: C:/OSPanel/domains/ai_assistant/api/projects/4/data/graph_store.json
INFO:llama_index.indices.loading:Loading indices with ids: ['vector_index']
DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings
DEBUG:openai:api_version=None data='{"input": ["hello"], "model": "text-embedding-ada-002", "encoding_format": "base64"}' message='Post details'
DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/embeddings HTTP/1.1" 200 None
DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=ac662b2bde48aa0aeb9233d2e5c5d357 response_code=200
DEBUG:llama_index.indices.utils:> Top 1 nodes:
> [Node 909c5dd5-e87e-4999-a794-761f96e44168] [Similarity score:             -0.0301768] Welcome to TechHub, your ultimate destination for cutting-edge technology and innovation! At Tech...
DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions
DEBUG:openai:api_version=None data='{"messages": [{"role": "system", "content": "Context information is below.\\n--------------------\\nWelcome to TechHub, your ultimate destination for cutting-edge technology and innovation! At TechHub, we pride ourselves on being a premier online store dedicated to providing a curated selection of the latest and greatest in tech gadgets, including laptops, smartphones, and more.\\n\\nAbout Us:\\n\\nAt the heart of TechHub is a passion for technology and a commitment to delivering top-notch products that seamlessly blend style, functionality, and performance. With a team of tech enthusiasts, we understand the ever-evolving nature of the digital landscape and strive to stay at the forefront of the latest trends and advancements.\\n\\nOur Mission:\\n\\nOur mission at TechHub is to empower our customers with the best tools and devices that enhance their digital lifestyle. Whether you\'re a professional looking for a high-performance laptop to boost productivity or a smartphone aficionado seeking the latest flagship model, we\'ve got you covered. We believe that technology should not only meet but exceed your expectations, making every interaction with your devices a seamless and enjoyable experience.\\n\\nQuality Assurance:\\n\\nTechHub is committed to delivering products of the highest quality. We carefully curate our inventory, partnering with leading brands known for their commitment to excellence. Each product undergoes rigorous testing to ensure reliability, durability, and top-notch performance. Our dedication to quality assurance is reflected in every item we offer.\\n\\nCustomer-Centric Approach:\\n\\nAt TechHub, our customers are at the core of everything we do. We prioritize your satisfaction and strive to provide an unparalleled shopping experience. Our customer support team is ready to assist you at every step, from product inquiries to after-sales support. We value your feedback and continuously work to improve our services based on your needs.\\n\\nSecure Shopping:\\n\\nYour online security is our priority. TechHub employs state-of-the-art encryption technology to ensure that your personal information remains secure and protected. Shop with confidence, knowing that your data is in safe hands.\\n\\nExplore the world of innovation at TechHub, where technology meets lifestyle. Discover the perfect blend of style and functionality with our carefully curated selection of laptops, smartphones, and other cutting-edge gadgets. Join us on this exciting journey, and let\'s embrace the future together!\\n--------------------\\n\\u0412\\u044b \\u2014 \\u0418\\u0418 \\u0430\\u0441\\u0441\\u0438\\u0441\\u0442\\u0435\\u043d\\u0442 \\u043a\\u043e\\u043c\\u043f\\u0430\\u043d\\u0438\\u0438. \\u0412\\u0430\\u043c \\u0431\\u0443\\u0434\\u0443\\u0442 \\u0437\\u0430\\u0434\\u0430\\u043d\\u044b \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u044b \\u043e \\u043d\\u0430\\u0448\\u0435\\u0439 \\u043a\\u043e\\u043c\\u043f\\u0430\\u043d\\u0438\\u0438. \\u0412\\u0435\\u0436\\u043b\\u0438\\u0432\\u043e \\u043e\\u0442\\u0432\\u0435\\u0447\\u0430\\u0439\\u0442\\u0435 \\u043d\\u0430 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u044b.\\r\\n    \\u041d\\u0438\\u0436\\u0435 \\u043c\\u044b \\u043f\\u0440\\u0435\\u0434\\u043e\\u0441\\u0442\\u0430\\u0432\\u0438\\u043b\\u0438 \\u043a\\u043e\\u043d\\u0442\\u0435\\u043a\\u0441\\u0442\\u043d\\u0443\\u044e \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u044e.\\r\\n    ---------------------\\r\\n    {context_str}\\r\\n    ---------------------\\r\\n    \\u0423\\u0447\\u0438\\u0442\\u044b\\u0432\\u0430\\u044f \\u044d\\u0442\\u0443 \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u044e, \\u043e\\u0442\\u0432\\u0435\\u0442\\u044c\\u0442\\u0435, \\u043f\\u043e\\u0436\\u0430\\u043b\\u0443\\u0439\\u0441\\u0442\\u0430, \\u043d\\u0430 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441: {query_str}\\r\\n    . \\n Information about product"}, {"role": "user", "content": "hello"}], "stream": false, "model": "gpt-3.5-turbo", "temperature": 0, "max_tokens": null}' message='Post details'
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=543 request_id=f82e8c8c3ba3441012f98b70e0bd6f5b response_code=200
INFO:myapp:{'api': 1, 'product_data': '{"attributes": "color", "categories": "misc,smartphones"}', 'website': 'asdf', 'id': 4, 'left_tokens': 32345, 'description': None, 'temperature': Decimal('0.00'), 'model': 'gpt-3.5-turbo', 'prompt': 'Вы — ИИ ассистент компании. Вам будут заданы вопросы о нашей компании. Вежливо отвечайте на вопросы.\r\n    Ниже мы предоставили контекстную информацию.\r\n    ---------------------\r\n    {context_str}\r\n    ---------------------\r\n    Учитывая эту информацию, ответьте, пожалуйста, на вопрос: {query_str}\r\n    ', 'response_mode': 'compact', 'user_id': 12}
DEBUG:root:This is a debug message
DEBUG:celery.utils.functional:
def send_chat_request(user_id, project_id, session, total_tokens, answer):
    return 1

INFO:werkzeug:127.0.0.1 - - [31/Dec/2023 16:23:04] "POST /projects/e47f0f811145d03c33fd6639dc3f09c0839d0891e264d849a75df04f4d047625/wck69kcLPjX9CFsDqNKufQIoXWuONpeD HTTP/1.1" 200 -
